from datetime import datetime, timezone


import os
import shutil
import uuid
import json
from bson import ObjectId

from fastapi import APIRouter, Depends, HTTPException, UploadFile, File
from fastapi.responses import StreamingResponse, FileResponse
from pydantic import BaseModel
from motor.motor_asyncio import AsyncIOMotorDatabase
from app.db.mongo import get_db
from app.models.chat import ChatSession, ChatMessage, DocumentMetadata
from app.services.rag_service import rag_service
from app.services.ollama_service import ollama_service
from typing import Optional, List


def get_query(session_id: str) -> dict:
    """Helper to create MongoDB query for session_id (String or ObjectId)"""
    try:
        return {"_id": ObjectId(session_id), "is_deleted": False}
    except Exception:
        return {"_id": session_id, "is_deleted": False}


router = APIRouter(prefix="/chats", tags=["Chat"])


class CreateChatRequest(BaseModel):
    title: str | None = None


class RenameChatRequest(BaseModel):
    title: str


class MessageRequest(BaseModel):
    message: str
    use_mmr: bool | None = True  # Enable MMR by default
    top_k: int | None = 5  # Number of context chunks


@router.post("/", response_model=ChatSession, response_model_by_alias=True)
async def create_chat_session(
    request: CreateChatRequest,
    db: AsyncIOMotorDatabase = Depends(get_db)
):
    """Create a new chat session"""
    session = ChatSession(title=request.title or "New Chat")
    # Insert with _id generated by MongoDB or the model
    # model_dump(by_alias=True) will use "_id"
    doc = session.model_dump(by_alias=True, exclude_none=True)
    result = await db["sessions"].insert_one(doc)
    session.id = str(result.inserted_id)
    return session


@router.get("/", response_model=list[ChatSession], response_model_by_alias=True)
async def list_chat_sessions(
    skip: int = 0,
    limit: int = 20,
    db: AsyncIOMotorDatabase = Depends(get_db)
):
    """List all chat sessions (excluding deleted ones)"""
    cursor = db["sessions"].find({"is_deleted": False}).sort(
        "updated_at", -1).skip(skip).limit(limit)
    sessions = await cursor.to_list(length=limit)
    
    # Filter out deleted documents from each session
    for session in sessions:
        if "documents" in session:
            session["documents"] = [d for d in session["documents"] if not d.get("is_deleted", False)]
            
    return sessions


@router.get("/{session_id}", response_model=ChatSession, response_model_by_alias=True)
async def get_chat_session(
    session_id: str,
    db: AsyncIOMotorDatabase = Depends(get_db)
):
    """Get a specific chat session"""
    session = await db["sessions"].find_one(get_query(session_id))
    if not session:
        raise HTTPException(status_code=404, detail="Session not found")
    
    # Filter out deleted documents
    if "documents" in session:
        session["documents"] = [d for d in session["documents"] if not d.get("is_deleted", False)]
        
    return session


@router.delete("/{session_id}")
async def delete_chat_session(
    session_id: str,
    db: AsyncIOMotorDatabase = Depends(get_db)
):
    """Soft delete a chat session and clean up RAG data"""
    query = get_query(session_id)
    result = await db["sessions"].update_one(
        query,
        {
            "$set": {"is_deleted": True},
            "$currentDate": {"updated_at": True}
        }
    )
    if result.matched_count == 0:
        raise HTTPException(status_code=404, detail="Session not found")

    # Clean up RAG data (physical deletion from ChromaDB/storage)
    deletion_success = await rag_service.delete_session_data(session_id)

    return {
        "message": "Session deleted logically and RAG data cleaned up",
        "rag_cleanup_success": deletion_success
    }


@router.post("/{session_id}/upload")
async def upload_documents(
    session_id: str,
    files: Optional[List[UploadFile]] = File(default=None),
    db: AsyncIOMotorDatabase = Depends(get_db)
):
    """Upload documents to a chat session"""
    if not files:
        raise HTTPException(
            status_code=400,
            detail="No files uploaded or 'files' field missing"
        )

    # Get or create session
    try:
        query_id = ObjectId(session_id)
        session = await db["sessions"].find_one({"_id": query_id, "is_deleted": False})
    except Exception:
        session = await db["sessions"].find_one({"_id": session_id, "is_deleted": False})
        if session:
            query_id = session_id
        else:
            # Create a new session with an auto-generated ObjectId
            new_session = ChatSession(title="New Chat")
            doc = new_session.model_dump(by_alias=True, exclude_none=True)
            result = await db["sessions"].insert_one(doc)
            query_id = result.inserted_id
            session = doc

    # Get existing filenames to check for duplicates
    existing_filenames = {
        doc.get("filename")
        for doc in session.get("documents", [])
        if not doc.get("is_deleted", False)
    }

    uploaded_files = []
    total_chunks = 0
    failed_files = []
    skipped_files = []

    for file in files:
        # Check for duplicates BEFORE processing
        if file.filename in existing_filenames:
            print(f"Skipping duplicate file: {file.filename}")
            skipped_files.append({
                "filename": file.filename,
                "error": "File already exists in this session"
            })
            continue

        temp_path = f"temp_{uuid.uuid4()}_{file.filename}"

        try:
            # Save uploaded file temporarily
            with open(temp_path, "wb") as buffer:
                shutil.copyfileobj(file.file, buffer)

            try:
                # Ingest file with enhanced stats
                stats = await rag_service.ingest_file(
                    temp_path,
                    file.filename,
                    str(query_id)
                )
                # stats is now a dict, not an int
                chunk_count = stats.get(
                    "chunk_count", 0) if isinstance(stats, dict) else 0
                total_chunks += chunk_count

                # Save metadata with stats
                doc_meta = DocumentMetadata(
                    filename=file.filename,
                    content_type=file.content_type or "application/octet-stream",
                    size=file.size or 0
                )

                # Add stats to metadata dict
                doc_dict = doc_meta.dict()
                doc_dict["stats"] = stats
                uploaded_files.append(doc_dict)

                # Add to existing filenames set
                existing_filenames.add(file.filename)

            except FileExistsError:
                # This shouldn't happen now since we check above, but keep as safety
                print(f"File exists error for: {file.filename}")
                skipped_files.append({
                    "filename": file.filename,
                    "error": "File already exists in this session"
                })
                continue

            except Exception as e:
                print(f"Error ingesting {file.filename}: {e}")
                failed_files.append({
                    "filename": file.filename,
                    "error": str(e)
                })
                continue

        except Exception as e:
            print(f"Error uploading {file.filename}: {e}")
            failed_files.append({
                "filename": file.filename,
                "error": f"Upload failed: {str(e)}"
            })
        finally:
            # Clean up temp file
            if os.path.exists(temp_path):
                os.remove(temp_path)

    # Bulk update session with uploaded files
    if uploaded_files:
        await db["sessions"].update_one(
            {"_id": query_id},
            {
                "$push": {"documents": {"$each": uploaded_files}},
                "$currentDate": {"updated_at": True}
            }
        )

        # Add system message for successful uploads
        filenames = ", ".join([d["filename"] for d in uploaded_files])
        system_note = ChatMessage(
            role="system",
            content=f"User uploaded files: {filenames}"
        )
        await db["sessions"].update_one(
            {"_id": query_id},
            {"$push": {"messages": system_note.model_dump()}}
        )

    return {
        "session_id": str(query_id),
        "uploaded_count": len(uploaded_files),
        "skipped_count": len(skipped_files),
        "failed_count": len(failed_files),
        "total_chunks_ingested": total_chunks,
        "uploaded_files": [f["filename"] for f in uploaded_files],
        "skipped_files": skipped_files,
        "failed_files": failed_files
    }


@router.post("/{session_id}/message")
async def send_message(
    session_id: str,
    request: MessageRequest,
    db: AsyncIOMotorDatabase = Depends(get_db)
):
    """Send a message and get AI response with RAG context"""
    session_data = await db["sessions"].find_one(get_query(session_id))
    if not session_data:
        raise HTTPException(status_code=404, detail="Session not found")

    # Get list of available documents
    available_docs = [
        d.get("filename")
        for d in session_data.get("documents", [])
        if not d.get("is_deleted", False)
    ]

    # Retrieve relevant context using enhanced query
    context_texts = []

    if available_docs:
        try:
            # Query with MMR for better diversity
            related_docs = await rag_service.query(
                request.message,
                session_id,
                filenames=available_docs,
                k=request.top_k,
                use_mmr=request.use_mmr
            )

            # Add document list to system context
            doc_list_str = ", ".join(available_docs)
            context_texts.append(
                f"AVAILABLE DOCUMENTS: {doc_list_str}\n"
                f"Answer based on these documents when relevant."
            )

            # Format retrieved chunks with source citations
            if related_docs:
                context_texts.append("\nRELEVANT CONTEXT:")
                for idx, doc in enumerate(related_docs, 1):
                    source = doc.metadata.get("source", "Unknown")
                    chunk_idx = doc.metadata.get("chunk_index", "?")
                    context_texts.append(
                        f"\n[{idx}. Source: {source}, Chunk {chunk_idx}]\n{doc.page_content}"
                    )
        except Exception as e:
            print(f"Error retrieving context: {e}")
            # Continue without context if retrieval fails

    # Append user message to session
    user_msg = ChatMessage(role="user", content=request.message)
    await db["sessions"].update_one(
        get_query(session_id),
        {
            "$push": {"messages": user_msg.model_dump()},
            "$currentDate": {"updated_at": True}
        }
    )

    # Construct message history for LLM (last 10 messages)
    history = session_data.get("messages", [])[-10:]
    history.append(user_msg.model_dump())

    # Stream response
    async def event_generator():
        full_response = ""
        try:
            async for token in ollama_service.stream_chat(history, context_texts):
                full_response += token
                yield f"data: {json.dumps({'token': token})}\n\n"

            # Save assistant message
            assistant_msg = ChatMessage(
                role="assistant", content=full_response)
            await db["sessions"].update_one(
                get_query(session_id),
                {
                    "$push": {"messages": assistant_msg.model_dump()},
                    "$currentDate": {"updated_at": True}
                }
            )
            yield "data: [DONE]\n\n"

        except Exception as e:
            error_msg = f"Error generating response: {str(e)}"
            print(error_msg)
            yield f"data: {json.dumps({'error': error_msg})}\n\n"
            yield "data: [DONE]\n\n"

    return StreamingResponse(event_generator(), media_type="text/event-stream")


@router.delete("/{session_id}/documents/{filename}")
async def delete_document(
    session_id: str,
    filename: str,
    db: AsyncIOMotorDatabase = Depends(get_db)
):
    """Soft delete a specific document from a session and remove from ChromaDB"""
    query = get_query(session_id)
    session = await db["sessions"].find_one(query)
    if not session:
        raise HTTPException(status_code=404, detail="Session not found")

    # Mark specifically the non-deleted document as deleted in MongoDB
    result = await db["sessions"].update_one(
        {
            "_id": session["_id"],
            "documents": {
                "$elemMatch": {
                    "filename": filename,
                    "is_deleted": False
                }
            }
        },
        {
            "$set": {"documents.$.is_deleted": True},
            "$currentDate": {"updated_at": True}
        }
    )

    if result.matched_count == 0:
        raise HTTPException(status_code=404, detail="Document not found")

    # Delete from RAG (remove from ChromaDB collection + physical file)
    deletion_success = await rag_service.delete_document(session_id, filename)

    # Add system message
    system_note = ChatMessage(
        role="system",
        content=f"User removed file: {filename}"
    )
    await db["sessions"].update_one(
        query,
        {"$push": {"messages": system_note.model_dump()}}
    )

    return {
        "message": f"Document {filename} deleted logically and RAG data cleaned up",
        "deletion_success": deletion_success
    }


@router.patch("/{session_id}")
async def rename_chat_session(
    session_id: str,
    request: RenameChatRequest,
    db: AsyncIOMotorDatabase = Depends(get_db)
):
    """Rename a chat session"""
    result = await db["sessions"].update_one(
        get_query(session_id),
        {
            "$set": {"title": request.title},
            "$currentDate": {"updated_at": True}
        }
    )
    if result.matched_count == 0:
        raise HTTPException(status_code=404, detail="Session not found")

    return {"id": session_id, "title": request.title}


@router.get("/{session_id}/documents/{filename}")
async def get_document_file(
    session_id: str,
    filename: str,
    db: AsyncIOMotorDatabase = Depends(get_db)
):
    """Download/view a document file"""
    # Verify session exists
    session = await db["sessions"].find_one(get_query(session_id))
    if not session:
        raise HTTPException(status_code=404, detail="Session not found")

    # Verify document exists in session and is not deleted
    doc_exists = any(
        d.get("filename") == filename and not d.get("is_deleted", False)
        for d in session.get("documents", [])
    )
    if not doc_exists:
        raise HTTPException(
            status_code=404,
            detail="Document not found or deleted"
        )

    storage_path = os.path.join("storage", session_id, "documents", filename)

    if not os.path.exists(storage_path):
        raise HTTPException(status_code=404, detail="File not found on disk")

    try:
        # Ensure absolute path for safety
        abs_path = os.path.abspath(storage_path)

        # Determine media type based on file extension
        media_type = "application/pdf"
        if filename.lower().endswith(".txt"):
            media_type = "text/plain"
        elif filename.lower().endswith(".json"):
            media_type = "application/json"
        elif filename.lower().endswith(".csv"):
            media_type = "text/csv"

        return FileResponse(
            abs_path,
            filename=filename,
            media_type=media_type,
            content_disposition_type="inline"
        )
    except Exception as e:
        print(f"Error serving file {filename}: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Could not serve file: {str(e)}"
        )


@router.get("/{session_id}/documents/{filename}/stats")
async def get_document_stats(
    session_id: str,
    filename: str,
    db: AsyncIOMotorDatabase = Depends(get_db)
):
    """Get statistics about a document's chunks and embeddings"""
    session = await db["sessions"].find_one(get_query(session_id))
    if not session:
        raise HTTPException(status_code=404, detail="Session not found")

    stats = await rag_service.get_document_stats(session_id, filename)

    if not stats:
        raise HTTPException(
            status_code=404,
            detail="Could not retrieve document statistics"
        )

    return stats


@router.get("/{session_id}/documents")
async def list_session_documents(
    session_id: str,
    db: AsyncIOMotorDatabase = Depends(get_db)
):
    """List all documents in a session with their metadata (excluding deleted)"""
    session = await db["sessions"].find_one(get_query(session_id))
    if not session:
        raise HTTPException(status_code=404, detail="Session not found")

    documents = [d for d in session.get("documents", []) if not d.get("is_deleted", False)]

    return {
        "session_id": session_id,
        "document_count": len(documents),
        "documents": documents
    }
